name: Weekly Analytics Feedback - Learn from Our Performance

# Runs weekly to analyze OUR video performance and adjust strategy
# Fully autonomous - no human intervention needed!

on:
  # Weekly on Sunday at 2 AM UTC
  schedule:
    - cron: '0 2 * * 0'
  
  # Manual trigger for testing
  workflow_dispatch:

jobs:
  analyze-our-performance:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Restore persistent state
        uses: dawidd6/action-download-artifact@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          name: persistent-state
          path: data/persistent/
          if_no_artifact_found: ignore
          search_artifacts: true
        continue-on-error: true

      - name: Create directories
        run: mkdir -p data/persistent

      # ================================================================
      # FETCH PERFORMANCE DATA FROM OUR YOUTUBE CHANNEL
      # ================================================================
      - name: Fetch Our Video Performance
        run: |
          python << 'EOF'
          import os
          import json
          import requests
          from datetime import datetime, timedelta
          from pathlib import Path

          YOUTUBE_CLIENT_ID = os.environ.get("YOUTUBE_CLIENT_ID")
          YOUTUBE_CLIENT_SECRET = os.environ.get("YOUTUBE_CLIENT_SECRET")
          YOUTUBE_REFRESH_TOKEN = os.environ.get("YOUTUBE_REFRESH_TOKEN")
          GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

          ANALYTICS_FILE = Path("data/persistent/analytics_state.json")
          VARIETY_FILE = Path("data/persistent/variety_state.json")
          VIRAL_PATTERNS_FILE = Path("data/persistent/viral_patterns.json")

          def safe_print(msg):
              try: print(msg)
              except: print(msg.encode('ascii', 'ignore').decode())

          def get_access_token():
              """Get fresh YouTube access token."""
              if not all([YOUTUBE_CLIENT_ID, YOUTUBE_CLIENT_SECRET, YOUTUBE_REFRESH_TOKEN]):
                  return None
              
              try:
                  response = requests.post(
                      "https://oauth2.googleapis.com/token",
                      data={
                          "client_id": YOUTUBE_CLIENT_ID,
                          "client_secret": YOUTUBE_CLIENT_SECRET,
                          "refresh_token": YOUTUBE_REFRESH_TOKEN,
                          "grant_type": "refresh_token",
                      },
                      timeout=15
                  )
                  if response.status_code == 200:
                      return response.json().get("access_token")
              except Exception as e:
                  safe_print(f"[!] Token error: {e}")
              return None

          def get_our_videos(access_token, max_results=50):
              """Get our channel's recent videos."""
              if not access_token:
                  return []
              
              headers = {"Authorization": f"Bearer {access_token}"}
              
              try:
                  # Get our channel
                  response = requests.get(
                      "https://www.googleapis.com/youtube/v3/channels",
                      params={"part": "contentDetails", "mine": "true"},
                      headers=headers,
                      timeout=15
                  )
                  
                  if response.status_code != 200:
                      safe_print(f"[!] Channel fetch failed: {response.status_code}")
                      return []
                  
                  data = response.json()
                  if not data.get("items"):
                      return []
                  
                  uploads_playlist = data["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
                  
                  # Get videos from playlist
                  response = requests.get(
                      "https://www.googleapis.com/youtube/v3/playlistItems",
                      params={
                          "part": "snippet",
                          "playlistId": uploads_playlist,
                          "maxResults": max_results
                      },
                      headers=headers,
                      timeout=15
                  )
                  
                  if response.status_code != 200:
                      return []
                  
                  videos = []
                  for item in response.json().get("items", []):
                      videos.append({
                          "id": item["snippet"]["resourceId"]["videoId"],
                          "title": item["snippet"]["title"],
                          "published": item["snippet"]["publishedAt"]
                      })
                  
                  return videos
              except Exception as e:
                  safe_print(f"[!] Get videos error: {e}")
                  return []

          def get_video_stats(access_token, video_ids):
              """Get stats for our videos."""
              if not access_token or not video_ids:
                  return {}
              
              headers = {"Authorization": f"Bearer {access_token}"}
              
              try:
                  response = requests.get(
                      "https://www.googleapis.com/youtube/v3/videos",
                      params={
                          "part": "statistics",
                          "id": ",".join(video_ids[:50])
                      },
                      headers=headers,
                      timeout=15
                  )
                  
                  if response.status_code == 200:
                      stats = {}
                      for item in response.json().get("items", []):
                          stats[item["id"]] = {
                              "views": int(item["statistics"].get("viewCount", 0)),
                              "likes": int(item["statistics"].get("likeCount", 0)),
                              "comments": int(item["statistics"].get("commentCount", 0))
                          }
                      return stats
              except Exception as e:
                  safe_print(f"[!] Stats error: {e}")
              return {}

          def analyze_performance_with_ai(videos_with_stats):
              """AI performs MICRO-LEVEL analysis of OUR performance.
              Analyzes EVERYTHING: topics, titles, tricks, baits, hacks, engagement patterns.
              """
              if not GROQ_API_KEY or not videos_with_stats:
                  return {}
              
              prompt = f"""You are our viral video expert. Perform DEEP MICRO-ANALYSIS of our YouTube Shorts:

          OUR VIDEO DATA (with performance stats):
          {json.dumps(videos_with_stats[:15], indent=2)}

          MICRO-ANALYZE EVERYTHING for maximum virality:

          1. **CATEGORY PERFORMANCE**: Which categories got most views?
          2. **TITLE TRICKS**: What title structures performed best? Numbers, power words, curiosity gaps
          3. **HOOK PATTERNS**: What hooks stopped the scroll? Questions? Shocking statements? Countdowns?
          4. **ENGAGEMENT BAITS**: What drove comments? Questions? Challenges? Controversies?
          5. **PSYCHOLOGICAL TRIGGERS**: Fear of missing out? Curiosity? Surprise? Validation?
          6. **MUSIC MOOD CORRELATION**: What moods correlate with high engagement?
          7. **VOICE STYLES**: Energetic? Calm? Mysterious? Which worked best?
          8. **CONTENT THEMES**: What specific angles within categories work?
          9. **VIRALITY HACKS**: Any patterns in our best videos that made them spread?
          10. **CALL-TO-ACTIONS**: What CTAs drove most engagement?
          11. **POSTING TIME ANALYSIS**: Based on publish dates, what day/time seems to work best?
          12. **TITLE STYLE ANALYSIS**: If detectable, which title styles (number_hook, curiosity_gap, shocking) perform best?

          Return comprehensive JSON with GOLD-VALUE insights:
          {{
              "best_performing_categories": ["category1", "category2"],
              "title_tricks_that_work": ["use numbers", "curiosity gaps", ...],
              "best_title_patterns": ["pattern with {{placeholder}}", ...],
              "optimal_title_word_count": number,
              "optimal_duration": number (ideal video length in seconds, 15-30),
              "optimal_phrase_count": number (ideal phrases per video, 3-6),
              "optimal_words_per_phrase": number (ideal words, 8-18),
              "hook_types_that_work": ["question hook", "shocking statement", ...],
              "psychological_triggers": ["curiosity", "fomo", ...],
              "engagement_baits": ["comment if...", "type 1 if...", ...],
              "best_music_moods": ["dramatic", "mysterious", ...],
              "best_voice_styles": ["energetic", "calm", ...],
              "best_content_themes": ["money secrets", "psychology tricks", ...],
              "virality_hacks": ["specific hack 1", "specific hack 2", ...],
              "do_more": ["specific action 1", "specific action 2"],
              "avoid": ["specific thing to avoid 1", ...],
              "next_video_suggestions": ["very specific topic 1", "topic 2", "topic 3"],
              "category_weights": {{"category1": 0.3, "category2": 0.25, ...}},
              "key_insight": "One sentence summary of most important finding",
              "priority_improvements": ["highest priority", "second priority", ...],
              "best_posting_days": ["Monday", "Friday", ...],
              "best_posting_hours_utc": [14, 18, ...],
              "best_title_styles": ["number_hook", "curiosity_gap", ...]
          }}

          Be SPECIFIC and ACTIONABLE based on ACTUAL numbers. JSON ONLY."""

              try:
                  response = requests.post(
                      "https://api.groq.com/openai/v1/chat/completions",
                      headers={
                          "Authorization": f"Bearer {GROQ_API_KEY}",
                          "Content-Type": "application/json"
                      },
                      json={
                          "model": "llama-3.3-70b-versatile",
                          "messages": [{"role": "user", "content": prompt}],
                          "temperature": 0.7,
                          "max_tokens": 800
                      },
                      timeout=30
                  )
                  
                  if response.status_code == 200:
                      content = response.json()["choices"][0]["message"]["content"]
                      import re
                      match = re.search(r'\{[\s\S]*\}', content)
                      if match:
                          return json.loads(match.group())
              except Exception as e:
                  safe_print(f"[!] AI analysis error: {e}")
              return {}

          # ================================================================
          # MAIN FLOW
          # ================================================================
          safe_print("=" * 60)
          safe_print("WEEKLY ANALYTICS FEEDBACK")
          safe_print(f"Date: {datetime.now().isoformat()}")
          safe_print("=" * 60)

          # Get access token
          access_token = get_access_token()
          if not access_token:
              safe_print("[!] Could not get YouTube access token")
              exit(0)

          safe_print("[OK] YouTube authenticated")

          # Get our videos
          our_videos = get_our_videos(access_token, max_results=50)
          safe_print(f"[OK] Found {len(our_videos)} videos")

          if not our_videos:
              safe_print("[!] No videos found - nothing to analyze")
              exit(0)

          # Get stats
          video_ids = [v["id"] for v in our_videos]
          stats = get_video_stats(access_token, video_ids)
          safe_print(f"[OK] Got stats for {len(stats)} videos")

          # Merge stats with video data
          for video in our_videos:
              if video["id"] in stats:
                  video.update(stats[video["id"]])

          # Sort by views
          our_videos.sort(key=lambda x: x.get("views", 0), reverse=True)

          safe_print("\n[TOP PERFORMERS]")
          for i, v in enumerate(our_videos[:5], 1):
              safe_print(f"  {i}. {v.get('views', 0):,} views - {v.get('title', 'N/A')[:50]}")

          # AI Analysis
          safe_print("\n[AI ANALYSIS]")
          analysis = analyze_performance_with_ai(our_videos)

          if analysis:
              safe_print(f"  Key insight: {analysis.get('key_insight', 'N/A')}")
              safe_print(f"  Best categories: {analysis.get('best_performing_categories', [])}")
              safe_print(f"  Do more: {analysis.get('do_more', [])[:2]}")
              
              # Update variety state with ALL learned preferences (MICRO-LEVEL)
              variety_state = {}
              if VARIETY_FILE.exists():
                  try:
                      with open(VARIETY_FILE, 'r') as f:
                          variety_state = json.load(f)
                  except:
                      pass
              
              # Category weights from performance
              if analysis.get("category_weights"):
                  variety_state["learned_weights"] = analysis["category_weights"]
              
              if analysis.get("best_performing_categories"):
                  variety_state["preferred_categories"] = analysis["best_performing_categories"]
              
              if analysis.get("avoid"):
                  variety_state["avoid_categories"] = analysis["avoid"]
              
              # MICRO-LEVEL preferences
              if analysis.get("best_music_moods"):
                  variety_state["preferred_music_moods"] = analysis["best_music_moods"]
              
              if analysis.get("best_voice_styles"):
                  variety_state["preferred_voice_styles"] = analysis["best_voice_styles"]
              
              if analysis.get("best_content_themes"):
                  variety_state["preferred_themes"] = analysis["best_content_themes"]
              
              if analysis.get("priority_improvements"):
                  variety_state["priority_improvements"] = analysis["priority_improvements"]
              
              # GOLD-VALUE: Tricks, Baits, Hacks
              if analysis.get("title_tricks_that_work"):
                  variety_state["title_tricks"] = analysis["title_tricks_that_work"]
              
              if analysis.get("hook_types_that_work"):
                  variety_state["hook_types"] = analysis["hook_types_that_work"]
              
              if analysis.get("psychological_triggers"):
                  variety_state["psych_triggers"] = analysis["psychological_triggers"]
              
              if analysis.get("engagement_baits"):
                  variety_state["engagement_baits"] = analysis["engagement_baits"]
              
              if analysis.get("virality_hacks"):
                  variety_state["virality_hacks"] = analysis["virality_hacks"]
              
              # AUDIENCE TARGETING: Optimal posting times
              if analysis.get("best_posting_days"):
                  variety_state["best_posting_days"] = analysis["best_posting_days"]
              
              if analysis.get("best_posting_hours_utc"):
                  variety_state["best_posting_hours_utc"] = analysis["best_posting_hours_utc"]
              
              # A/B TITLE TESTING: Learn which title styles work
              if analysis.get("best_title_styles"):
                  variety_state["best_title_styles"] = analysis["best_title_styles"]
              
              variety_state["last_feedback"] = datetime.now().isoformat()
              
              with open(VARIETY_FILE, 'w') as f:
                  json.dump(variety_state, f, indent=2)
              
              # Update viral patterns with ALL micro-level patterns
              viral_patterns = {}
              if VIRAL_PATTERNS_FILE.exists():
                  try:
                      with open(VIRAL_PATTERNS_FILE, 'r') as f:
                          viral_patterns = json.load(f)
                  except:
                      pass
              
              if analysis.get("best_title_patterns"):
                  existing = viral_patterns.get("our_best_patterns", [])
                  for p in analysis["best_title_patterns"]:
                      if p not in existing:
                          existing.append(p)
                  viral_patterns["our_best_patterns"] = existing[-10:]
              
              if analysis.get("next_video_suggestions"):
                  viral_patterns["ai_suggested_topics"] = analysis["next_video_suggestions"]
              
              if analysis.get("engagement_insights"):
                  viral_patterns["engagement_insights"] = analysis["engagement_insights"]
              
              if analysis.get("optimal_title_word_count"):
                  viral_patterns["optimal_title_word_count"] = analysis["optimal_title_word_count"]
              
              # HYBRID LEARNING: Optimal video metrics (with guardrails applied in generator)
              if analysis.get("optimal_duration"):
                  viral_patterns["optimal_duration"] = analysis["optimal_duration"]
              
              if analysis.get("optimal_phrase_count"):
                  viral_patterns["optimal_phrase_count"] = analysis["optimal_phrase_count"]
              
              if analysis.get("optimal_words_per_phrase"):
                  viral_patterns["optimal_words_per_phrase"] = analysis["optimal_words_per_phrase"]
              
              viral_patterns["last_performance_update"] = datetime.now().isoformat()
              viral_patterns["key_insight"] = analysis.get("key_insight", "")
              
              with open(VIRAL_PATTERNS_FILE, 'w') as f:
                  json.dump(viral_patterns, f, indent=2)
              
              # Save full analytics
              analytics_state = {
                  "videos": our_videos[:50],
                  "analysis": analysis,
                  "last_updated": datetime.now().isoformat(),
                  "total_views": sum(v.get("views", 0) for v in our_videos),
                  "avg_views": sum(v.get("views", 0) for v in our_videos) / len(our_videos) if our_videos else 0
              }
              
              with open(ANALYTICS_FILE, 'w') as f:
                  json.dump(analytics_state, f, indent=2)
              
              safe_print("\n[SAVED] Analytics feedback saved!")
              safe_print("  -> variety_state.json updated with learned preferences")
              safe_print("  -> viral_patterns.json updated with best patterns")
              safe_print("  -> Video generator will use these automatically!")
          else:
              safe_print("[!] AI analysis returned no results")

          safe_print("\n" + "=" * 60)
          safe_print("FEEDBACK COMPLETE - Next generation will use learnings!")
          safe_print("=" * 60)
          EOF
        env:
          YOUTUBE_CLIENT_ID: ${{ secrets.YOUTUBE_CLIENT_ID }}
          YOUTUBE_CLIENT_SECRET: ${{ secrets.YOUTUBE_CLIENT_SECRET }}
          YOUTUBE_REFRESH_TOKEN: ${{ secrets.YOUTUBE_REFRESH_TOKEN }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}

      - name: Save updated state
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: persistent-state
          path: data/persistent/
          retention-days: 30
          overwrite: true

      - name: Summary
        run: |
          echo "## Weekly Analytics Feedback Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### The Autonomous Feedback Loop" >> $GITHUB_STEP_SUMMARY
          echo "1. Fetched performance data from our YouTube channel" >> $GITHUB_STEP_SUMMARY
          echo "2. AI analyzed what's working and what's not" >> $GITHUB_STEP_SUMMARY
          echo "3. Updated variety preferences (category weights)" >> $GITHUB_STEP_SUMMARY
          echo "4. Updated viral patterns (best title patterns)" >> $GITHUB_STEP_SUMMARY
          echo "5. Next video generation will use these automatically!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### No Human Intervention Needed!" >> $GITHUB_STEP_SUMMARY
          echo "This workflow runs every Sunday and feeds learnings back." >> $GITHUB_STEP_SUMMARY

