#!/usr/bin/env python3
"""
ViralShorts Factory - Self-Learning Engine v15.0
=================================================

This module implements an AI-driven self-learning system that:
1. Tracks successful patterns from high-scoring videos
2. Learns from failures to avoid repeating mistakes
3. Adapts prompts based on what works
4. Provides real-time recommendations for content generation

The goal: Every video gets BETTER than the last!

Data Sources:
- Video metadata and scores from analytics_feedback.py
- First-attempt quality history from token_budget_manager.py
- Persistent state from GitHub Artifacts

Learning Strategy:
1. Score-based learning: What patterns correlate with high scores?
2. Engagement-based learning: What patterns get more views/likes?
3. Regeneration avoidance: What patterns avoid regenerations?
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import re

# State directory
STATE_DIR = Path("data/persistent")
STATE_DIR.mkdir(parents=True, exist_ok=True)

LEARNING_FILE = STATE_DIR / "self_learning.json"


@dataclass
class LearningPattern:
    """A pattern learned from video performance."""
    pattern_type: str  # "hook", "topic", "category", "phrase_style", "number_format"
    pattern_value: str  # The actual pattern
    success_count: int
    failure_count: int
    avg_score: float
    last_used: str
    examples: List[str]


class SelfLearningEngine:
    """
    Self-learning engine that improves video quality over time.
    
    Learns from:
    - High-scoring videos (8+/10)
    - Low-scoring videos (<6/10)
    - Regeneration patterns
    - View/engagement data
    """
    
    def __init__(self):
        self.data = self._load()
    
    def _load(self) -> Dict:
        """Load learning data from disk."""
        try:
            if LEARNING_FILE.exists():
                with open(LEARNING_FILE, 'r') as f:
                    return json.load(f)
        except Exception as e:
            print(f"[SelfLearning] Load error: {e}")
        
        # v17.8: Empty default - ALL values come from real analytics, not hardcoded
        # v17.9.11: Added title_patterns, hashtags, ctas for learning
        return {
            "source": "REAL_ANALYTICS",
            "patterns": {
                "hooks": [],      # Populated by learn_from_video()
                "topics": [],     # Populated by learn_from_video()
                "categories": [], # Populated by learn_from_video()
                "phrases": [],    # Populated by learn_from_video()
                "numbers": [],    # Populated by learn_from_video()
                "failures": [],   # Populated by learn_from_video()
            },
            "stats": {
                "total_videos": 0,
                "avg_first_attempt_score": None,  # Will be calculated
                "regeneration_rate": None,        # Will be calculated
                "top_categories": [],
                "worst_categories": [],
            },
            "recommendations": {
                "boost_phrases": [],     # Generated by generate_recommendations()
                "avoid_phrases": [],     # Generated by generate_recommendations()
                "best_hook_styles": [],  # Generated by generate_recommendations()
                "optimal_length": None,  # v17.8: Learned, not hardcoded
            },
            # v17.9.11: NEW - Learn from analytics which patterns work best
            "title_patterns": {
                "best": [],          # Title formats that got most views/CTR
                "worst": [],         # Title formats that underperformed
                "by_category": {},   # {category: [best patterns]}
            },
            "hashtag_performance": {
                "best": [],          # Hashtags that drove engagement
                "worst": [],         # Hashtags that hurt reach
                "trending": [],      # Currently trending hashtags
            },
            "cta_effectiveness": {
                "best": [],          # CTAs that got most comments/engagement
                "worst": [],         # CTAs that didn't work
                "by_goal": {},       # {goal: [best CTAs]} e.g. "subscribe", "comment"
            },
            "awaiting_real_data": True,
            "last_updated": datetime.now().isoformat()
        }
    
    def _save(self):
        """Save learning data to disk."""
        try:
            self.data["last_updated"] = datetime.now().isoformat()
            with open(LEARNING_FILE, 'w') as f:
                json.dump(self.data, f, indent=2)
        except Exception as e:
            print(f"[SelfLearning] Save error: {e}")
    
    def learn_from_video(self, 
                          score: int, 
                          category: str, 
                          topic: str,
                          hook: str, 
                          phrases: List[str],
                          was_regeneration: bool = False):
        """
        Learn from a generated video.
        
        Args:
            score: Quality score (1-10)
            category: Video category
            topic: Specific topic
            hook: First phrase (hook)
            phrases: All phrases
            was_regeneration: True if this was a regeneration
        """
        # Update stats
        self.data["stats"]["total_videos"] += 1
        
        # Extract patterns
        hook_pattern = self._extract_hook_pattern(hook)
        number_patterns = self._extract_number_patterns(phrases)
        phrase_style = self._analyze_phrase_style(phrases)
        
        if score >= 8:
            # SUCCESS - Learn what works
            self._record_success("hooks", hook_pattern, hook[:50])
            self._record_success("categories", category, topic[:30])
            self._record_success("phrases", phrase_style, hook[:50])
            for np in number_patterns:
                self._record_success("numbers", np["type"], np["example"])
            
            # Update best categories
            if category not in self.data["stats"]["top_categories"]:
                self.data["stats"]["top_categories"].append(category)
                self.data["stats"]["top_categories"] = self.data["stats"]["top_categories"][-10:]
                
        elif score < 6:
            # FAILURE - Learn what to avoid
            self._record_failure("failures", hook_pattern, hook[:50])
            self._record_failure("failures", category, topic[:30])
            
            # Update worst categories
            if category not in self.data["stats"]["worst_categories"]:
                self.data["stats"]["worst_categories"].append(category)
                self.data["stats"]["worst_categories"] = self.data["stats"]["worst_categories"][-5:]
        
        # Update regeneration rate
        if was_regeneration:
            total = self.data["stats"]["total_videos"]
            old_rate = self.data["stats"]["regeneration_rate"]
            self.data["stats"]["regeneration_rate"] = (
                (old_rate * (total - 1) + 1.0) / total
            )
        
        # Update recommendations based on learning
        self._update_recommendations()
        
        self._save()
        print(f"[SelfLearning] Learned from video: score={score}, category={category}")
    
    def _extract_hook_pattern(self, hook: str) -> str:
        """Extract the pattern type from a hook."""
        hook_lower = hook.lower()
        
        if '?' in hook:
            return "question"
        elif any(word in hook_lower for word in ["stop", "wait", "hold on", "listen"]):
            return "pattern_interrupt"
        elif any(word in hook_lower for word in ["99%", "most people", "nobody", "everyone"]):
            return "social_proof"
        elif any(word in hook_lower for word in ["secret", "truth", "hidden", "won't tell"]):
            return "mystery"
        elif any(word in hook_lower for word in ["shocking", "unbelievable", "crazy", "insane"]):
            return "shock"
        elif re.search(r'\$\d+|^\d+%|\d+ (times|days|hours)', hook_lower):
            return "specific_number"
        else:
            return "statement"
    
    def _extract_number_patterns(self, phrases: List[str]) -> List[Dict]:
        """Extract number patterns from phrases."""
        patterns = []
        full_text = " ".join(phrases)
        
        # Round numbers like $500, 80%, 1000
        if re.search(r'\$\d+00\b|\b\d0%\b|\b\d+000\b', full_text):
            patterns.append({"type": "round_number", "example": re.findall(r'\$\d+00|\d0%|\d+000', full_text)[0]})
        
        # Specific believable numbers like 3x, 10 minutes, 5 steps
        if re.search(r'\b\d+x\b|\b\d+ (minutes?|hours?|steps?|days?)\b', full_text):
            match = re.findall(r'\d+x|\d+ (?:minutes?|hours?|steps?|days?)', full_text)
            if match:
                patterns.append({"type": "contextual_number", "example": match[0]})
        
        # Awkward numbers (to avoid)
        if re.search(r'\$\d{4,}\b|\d+\.\d+%', full_text):
            patterns.append({"type": "awkward_number", "example": "AVOID"})
        
        return patterns
    
    def _analyze_phrase_style(self, phrases: List[str]) -> str:
        """Analyze the overall phrase style."""
        avg_length = sum(len(p.split()) for p in phrases) / len(phrases) if phrases else 0
        
        if avg_length <= 8:
            return "punchy"
        elif avg_length <= 12:
            return "concise"
        else:
            return "verbose"
    
    def _record_success(self, pattern_type: str, pattern_value: str, example: str):
        """Record a successful pattern."""
        patterns = self.data["patterns"].get(pattern_type, [])
        
        # Find existing or create new
        existing = next((p for p in patterns if p.get("value") == pattern_value), None)
        
        if existing:
            existing["success_count"] = existing.get("success_count", 0) + 1
            existing["last_used"] = datetime.now().isoformat()
            if example not in existing.get("examples", []):
                existing["examples"] = (existing.get("examples", []) + [example])[-5:]
        else:
            patterns.append({
                "value": pattern_value,
                "success_count": 1,
                "failure_count": 0,
                "last_used": datetime.now().isoformat(),
                "examples": [example]
            })
        
        # Keep only top patterns
        patterns.sort(key=lambda x: x.get("success_count", 0), reverse=True)
        self.data["patterns"][pattern_type] = patterns[:20]
    
    def _record_failure(self, pattern_type: str, pattern_value: str, example: str):
        """Record a failed pattern."""
        patterns = self.data["patterns"].get(pattern_type, [])
        
        existing = next((p for p in patterns if p.get("value") == pattern_value), None)
        
        if existing:
            existing["failure_count"] = existing.get("failure_count", 0) + 1
            existing["last_used"] = datetime.now().isoformat()
        else:
            patterns.append({
                "value": pattern_value,
                "success_count": 0,
                "failure_count": 1,
                "last_used": datetime.now().isoformat(),
                "examples": [example]
            })
        
        self.data["patterns"][pattern_type] = patterns[:20]
    
    def _update_recommendations(self):
        """Update recommendations based on learned patterns."""
        # Best hook styles
        hooks = self.data["patterns"].get("hooks", [])
        self.data["recommendations"]["best_hook_styles"] = [
            h["value"] for h in hooks 
            if h.get("success_count", 0) >= 2
        ][:5]
        
        # Phrases to boost
        phrases = self.data["patterns"].get("phrases", [])
        self.data["recommendations"]["boost_phrases"] = [
            p["value"] for p in phrases 
            if p.get("success_count", 0) >= 2
        ][:5]
        
        # Phrases to avoid
        failures = self.data["patterns"].get("failures", [])
        self.data["recommendations"]["avoid_phrases"] = [
            f["value"] for f in failures 
            if f.get("failure_count", 0) >= 2
        ][:5]
    
    def get_prompt_boost(self) -> str:
        """Get prompt boost based on learning."""
        boost = """
=== SELF-LEARNING INSIGHTS ===
Based on analysis of our best-performing videos:

"""
        # Best hook styles
        if self.data["recommendations"]["best_hook_styles"]:
            boost += "BEST HOOK PATTERNS (use these!):\n"
            for style in self.data["recommendations"]["best_hook_styles"][:3]:
                boost += f"- {style}\n"
            boost += "\n"
        
        # Top categories
        if self.data["stats"]["top_categories"]:
            boost += f"TOP CATEGORIES: {', '.join(self.data['stats']['top_categories'][:5])}\n\n"
        
        # Things to avoid
        if self.data["recommendations"]["avoid_phrases"]:
            boost += "AVOID THESE (caused failures):\n"
            for phrase in self.data["recommendations"]["avoid_phrases"][:3]:
                boost += f"- {phrase}\n"
            boost += "\n"
        
        # Stats
        boost += f"""
PERFORMANCE STATS:
- Total videos analyzed: {self.data['stats']['total_videos']}
- Regeneration rate: {self.data['stats']['regeneration_rate']*100:.1f}%
- Goal: <10% regeneration rate
================================
"""
        return boost
    
    def should_avoid_category(self, category: str) -> bool:
        """Check if a category should be avoided."""
        return category in self.data["stats"]["worst_categories"]
    
    def get_recommended_categories(self) -> List[str]:
        """Get recommended categories based on learning."""
        return self.data["stats"]["top_categories"][:5]
    
    # =========================================================================
    # v17.9.11: NEW LEARNING METHODS FOR TITLES, HASHTAGS, CTAs
    # =========================================================================
    
    def learn_from_title(self, title: str, views: int, category: str):
        """Learn which title patterns drive views."""
        # Extract pattern from title
        pattern = self._extract_title_pattern(title)
        
        # Initialize if needed
        if "title_patterns" not in self.data:
            self.data["title_patterns"] = {"best": [], "worst": [], "by_category": {}}
        
        # Determine if good or bad based on views threshold
        avg_views = 1000  # Will be replaced with actual average
        if views > avg_views * 1.5:  # 50% above average
            self._add_to_learned("title_patterns", "best", pattern, views)
            # Also track by category
            if category not in self.data["title_patterns"]["by_category"]:
                self.data["title_patterns"]["by_category"][category] = []
            if pattern not in self.data["title_patterns"]["by_category"][category]:
                self.data["title_patterns"]["by_category"][category].append(pattern)
        elif views < avg_views * 0.5:  # 50% below average
            self._add_to_learned("title_patterns", "worst", pattern, views)
        
        self._save()
    
    def learn_from_hashtags(self, hashtags: List[str], engagement: int):
        """Learn which hashtags drive engagement."""
        if "hashtag_performance" not in self.data:
            self.data["hashtag_performance"] = {"best": [], "worst": [], "trending": []}
        
        avg_engagement = 100  # Will be replaced with actual average
        for tag in hashtags:
            if engagement > avg_engagement * 1.5:
                self._add_to_learned("hashtag_performance", "best", tag, engagement)
            elif engagement < avg_engagement * 0.3:
                self._add_to_learned("hashtag_performance", "worst", tag, engagement)
        
        self._save()
    
    def learn_from_cta(self, cta: str, comments: int, goal: str = "comment"):
        """Learn which CTAs are effective."""
        if "cta_effectiveness" not in self.data:
            self.data["cta_effectiveness"] = {"best": [], "worst": [], "by_goal": {}}
        
        avg_comments = 10  # Will be replaced with actual average
        if comments > avg_comments * 2:
            self._add_to_learned("cta_effectiveness", "best", cta, comments)
            # Track by goal
            if goal not in self.data["cta_effectiveness"]["by_goal"]:
                self.data["cta_effectiveness"]["by_goal"][goal] = []
            if cta not in self.data["cta_effectiveness"]["by_goal"][goal]:
                self.data["cta_effectiveness"]["by_goal"][goal].append(cta)
        elif comments < avg_comments * 0.2:
            self._add_to_learned("cta_effectiveness", "worst", cta, comments)
        
        self._save()
    
    def _add_to_learned(self, category: str, list_name: str, value: str, score: int):
        """Add a value to a learned list with score."""
        if category not in self.data:
            self.data[category] = {}
        if list_name not in self.data[category]:
            self.data[category][list_name] = []
        
        # Check if already exists
        for item in self.data[category][list_name]:
            if item.get("value") == value:
                item["score"] = max(item.get("score", 0), score)
                item["count"] = item.get("count", 0) + 1
                return
        
        # Add new
        self.data[category][list_name].append({
            "value": value,
            "score": score,
            "count": 1
        })
        
        # Keep only top 20
        self.data[category][list_name] = sorted(
            self.data[category][list_name],
            key=lambda x: x.get("score", 0),
            reverse=True
        )[:20]
    
    def _extract_title_pattern(self, title: str) -> str:
        """Extract the pattern type from a title."""
        title_lower = title.lower()
        
        if "?" in title:
            return "question"
        elif any(word in title_lower for word in ["vs", "or", "versus"]):
            return "comparison"
        elif re.search(r'^\d+\s', title) or re.search(r'top \d+', title_lower):
            return "listicle"
        elif any(word in title_lower for word in ["how to", "why", "what"]):
            return "educational"
        elif any(word in title_lower for word in ["secret", "hidden", "truth"]):
            return "mystery"
        elif any(word in title_lower for word in ["shocking", "insane", "crazy", "impossible"]):
            return "shock"
        elif any(word in title_lower for word in ["this", "here's", "watch"]):
            return "direct"
        else:
            return "statement"
    
    def get_best_title_patterns(self, category: str = None) -> List[str]:
        """Get best performing title patterns."""
        if "title_patterns" not in self.data:
            return []
        
        if category and category in self.data["title_patterns"].get("by_category", {}):
            return self.data["title_patterns"]["by_category"][category][:3]
        
        return [p.get("value") for p in self.data["title_patterns"].get("best", [])][:5]
    
    def get_best_hashtags(self) -> List[str]:
        """Get best performing hashtags."""
        if "hashtag_performance" not in self.data:
            return ["#shorts"]  # Always safe default
        
        return [h.get("value") for h in self.data["hashtag_performance"].get("best", [])][:8]
    
    def get_best_ctas(self, goal: str = "comment") -> List[str]:
        """Get best performing CTAs for a goal."""
        if "cta_effectiveness" not in self.data:
            return []
        
        if goal in self.data["cta_effectiveness"].get("by_goal", {}):
            return self.data["cta_effectiveness"]["by_goal"][goal][:3]
        
        return [c.get("value") for c in self.data["cta_effectiveness"].get("best", [])][:5]


    def get_viral_triggers(self) -> Dict:
        """
        v17.9.16: Get viral triggers - dynamic, not hardcoded!
        
        Priority:
        1. Learned from analytics (best patterns)
        2. AI-generated (ask AI for current viral triggers)
        3. Never returns hardcoded defaults
        
        Returns dict with hook_triggers, engagement_triggers, etc.
        """
        # Try to get learned triggers first
        if self.data.get("patterns", {}).get("hooks"):
            learned_hook_triggers = [
                p.get("value", "") for p in self.data["patterns"]["hooks"]
                if p.get("success_count", 0) > p.get("failure_count", 0)
            ]
            if len(learned_hook_triggers) >= 3:
                return self._build_triggers_from_learned(learned_hook_triggers)
        
        # Otherwise, ask AI for current viral triggers
        return self._get_triggers_from_ai()
    
    def _build_triggers_from_learned(self, hook_patterns: List[str]) -> Dict:
        """Build triggers dict from learned patterns."""
        return {
            "source": "learned_from_analytics",
            "hook_triggers": hook_patterns[:5],
            "engagement_triggers": self.data.get("cta_effectiveness", {}).get("best", [])[:5],
            "power_words": self.data.get("recommendations", {}).get("boost_phrases", [])[:10],
            "avoid_words": self.data.get("recommendations", {}).get("avoid_phrases", [])[:5],
        }
    
    def _get_triggers_from_ai(self) -> Dict:
        """Ask AI for current viral triggers."""
        import os
        
        prompt = """You are a viral content expert. What are the TOP viral triggers for YouTube Shorts RIGHT NOW?

Based on current trends and algorithm behavior, provide:

1. HOOK TRIGGERS: 5 words/phrases that STOP the scroll (e.g., "STOP", "Wait", "Did you know")
2. ENGAGEMENT TRIGGERS: 5 phrases that drive comments/likes (e.g., "Comment below", "Like if you agree")
3. POWER WORDS: 10 words that boost CTR (e.g., "secret", "truth", "hidden")
4. AVOID WORDS: 5 words that hurt performance (overused, spam-flagged)

CRITICAL: Base this on CURRENT trends, not generic advice. What's working NOW?

Return JSON ONLY:
{
    "hook_triggers": ["trigger1", "trigger2", "trigger3", "trigger4", "trigger5"],
    "engagement_triggers": ["phrase1", "phrase2", "phrase3", "phrase4", "phrase5"],
    "power_words": ["word1", "word2", "word3", "word4", "word5", "word6", "word7", "word8", "word9", "word10"],
    "avoid_words": ["word1", "word2", "word3", "word4", "word5"]
}"""
        
        result = None
        
        # Try Groq first
        groq_key = os.environ.get("GROQ_API_KEY")
        if groq_key:
            try:
                from groq import Groq
                try:
                    from src.ai.model_helper import get_dynamic_groq_model
                    model = get_dynamic_groq_model()
                except:
                    model = "llama-3.3-70b-versatile"
                
                client = Groq(api_key=groq_key)
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=300,
                    temperature=0.5
                )
                result = response.choices[0].message.content
            except:
                pass
        
        # Try Gemini
        if not result:
            gemini_key = os.environ.get("GEMINI_API_KEY")
            if gemini_key:
                try:
                    import google.generativeai as genai
                    genai.configure(api_key=gemini_key)
                    from src.ai.model_helper import get_dynamic_gemini_model
                    model = genai.GenerativeModel(get_dynamic_gemini_model())
                    response = model.generate_content(prompt)
                    result = response.text
                except:
                    pass
        
        # Parse result
        if result:
            try:
                import json
                start = result.find('{')
                end = result.rfind('}') + 1
                if start >= 0 and end > start:
                    parsed = json.loads(result[start:end])
                    parsed["source"] = "ai_generated"
                    
                    # Cache the AI-generated triggers
                    self.data["ai_triggers"] = parsed
                    self.data["ai_triggers"]["generated_at"] = datetime.now().isoformat()
                    self._save()
                    
                    return parsed
            except:
                pass
        
        # If AI fails, use cached AI triggers if available
        if self.data.get("ai_triggers"):
            cached = self.data["ai_triggers"].copy()
            cached["source"] = "cached_ai"
            return cached
        
        # Ultimate fallback: empty (the caller should handle this)
        return {
            "source": "none_available",
            "hook_triggers": [],
            "engagement_triggers": [],
            "power_words": [],
            "avoid_words": []
        }


# Singleton instance
_learning_engine = None

def get_learning_engine() -> SelfLearningEngine:
    """Get the singleton learning engine."""
    global _learning_engine
    if _learning_engine is None:
        _learning_engine = SelfLearningEngine()
    return _learning_engine


if __name__ == "__main__":
    # Test the learning engine
    engine = get_learning_engine()
    
    # Simulate some learning
    engine.learn_from_video(
        score=9,
        category="productivity",
        topic="Morning routines that work",
        hook="STOP - This simple habit changed my life",
        phrases=[
            "STOP - This simple habit changed my life",
            "Wake up 30 minutes earlier every day",
            "You'll get 3x more done before noon",
            "Comment if you'll try this tomorrow!"
        ]
    )
    
    engine.learn_from_video(
        score=4,
        category="crypto",
        topic="Bitcoin price prediction",
        hook="You're losing $3333 every year",
        phrases=[
            "You're losing $3333 every year",
            "Crypto fees are killing your profits",
            "Use this obscure exchange instead"
        ],
        was_regeneration=True
    )
    
    print("\n" + engine.get_prompt_boost())

